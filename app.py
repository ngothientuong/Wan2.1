from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import torch
import os
import sys
import logging
import numpy as np
from datetime import datetime
import time
from threading import Thread
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torchvision.transforms.functional import resize
from wan.utils.utils import cache_video
from wan.utils.prompt_extend import DashScopePromptExpander, QwenPromptExpander

# ========== Logging Setup ==========
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)]
)

# ========== Initialize FastAPI App ==========
app = FastAPI(title="WAN 2.1 Optimized Text-to-Video API")

# ========== Multi-GPU & Memory Optimization ==========
NUM_GPUS = torch.cuda.device_count()
GPU_IDS = list(range(NUM_GPUS))

if NUM_GPUS > 0:
    logging.info(f"‚úÖ Multi-GPU enabled: {NUM_GPUS} GPUs detected.")
else:
    logging.info("‚ö†Ô∏è No GPU detected! Running on CPU.")

# ‚úÖ Use PyTorch CUDA Memory Optimizations
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.set_default_dtype(torch.float16)  # ‚úÖ Use FP16 for efficiency
torch.backends.cuda.matmul.allow_tf32 = True  # ‚úÖ Enable TF32 matmul optimization

# ========== Load Model Once for Reuse ==========
MODEL_CACHE = {}

def load_model(task, ckpt_dir):
    """Loads WAN 2.1 model to an available GPU or CPU with full optimization."""
    if task not in MODEL_CACHE:
        device = f"cuda:{GPU_IDS[0]}" if NUM_GPUS > 0 else "cpu"
        logging.info(f"üîÑ Loading model {task} on {device} from {ckpt_dir}...")

        with torch.device(device), torch.amp.autocast(device):
            model = torch.hub.load("WAN-2.1", model=task).to(device).half()  # ‚úÖ Convert to FP16
            model = torch.compile(model, mode="max-autotune")  # ‚úÖ Enable dynamic optimizations

        # ‚úÖ Use Fully Sharded Data Parallel (FSDP) for Multi-GPU
        if NUM_GPUS > 1:
            model = FSDP(model)

        MODEL_CACHE[task] = model
    return MODEL_CACHE[task]

# ========== Health Check Endpoint ==========
@app.get("/health")
async def health_check():
    return JSONResponse(content={"status": "healthy", "gpus_available": NUM_GPUS})

# ========== Request Schema ==========
class VideoRequest(BaseModel):
    task: str
    prompt: str
    size: str = "1280*720"
    num_frames: int = 160  # ‚úÖ Optimized for 10s per request
    fps: int = 16
    seed: int = 42
    offload_model: bool = True
    t5_cpu: bool = True
    sample_shift: int = 3  # ‚úÖ Optimized for speed
    sample_guide_scale: float = 3.0
    use_prompt_extend: bool = False
    prompt_extend_method: str = "dashscope"
    prompt_extend_target_lang: str = "en"
    ckpt_dir: str

# ========== Optimized AI-Based Interpolation ==========
def interpolate_frames(frames, target_fps):
    """Interpolates frames using GPU-accelerated TorchVision resizing instead of CPU OpenCV."""
    logging.info(f"üé• Interpolating frames to {target_fps} FPS using GPU acceleration...")

    num_frames, height, width, _ = frames.shape
    interpolated_video = []

    for i in range(num_frames - 1):
        interpolated_video.append(frames[i])
        mid_frame = resize(torch.tensor(frames[i]).cuda(), (height, width))
        interpolated_video.append(mid_frame.cpu().numpy())

    interpolated_video.append(frames[-1])
    return np.array(interpolated_video, dtype=np.uint8)

# ========== Optimized Generate Video Function ==========
def generate_video(request: VideoRequest):
    start_time = time.time()
    model = load_model(request.task, request.ckpt_dir)

    # ‚úÖ Optimized Keyframe Generation
    keyframe_interval = 4
    num_keyframes = request.num_frames // keyframe_interval
    logging.info(f"‚ö° Generating {num_keyframes} keyframes instead of {request.num_frames} full frames.")

    # ‚úÖ Optimized Multi-GPU Processing with CUDA Streams & Graphs
    batch_size = max(1, num_keyframes // max(1, NUM_GPUS))
    keyframes = []

    cuda_graphs = [torch.cuda.CUDAGraph() for _ in GPU_IDS]
    streams = [torch.cuda.Stream(device=i) for i in GPU_IDS]

    for i in range(0, num_keyframes, batch_size):
        device_id = GPU_IDS[(i // batch_size) % NUM_GPUS] if NUM_GPUS > 0 else "cpu"
        model.to(device_id)

        with torch.cuda.graph(cuda_graphs[(i // batch_size) % NUM_GPUS]):
            with torch.cuda.stream(streams[(i // batch_size) % NUM_GPUS]):
                batch_output = model.generate(
                    request.prompt,
                    size=request.size,
                    frame_num=batch_size,
                    shift=request.sample_shift,
                    sample_solver="unipc",
                    sampling_steps=4,  # ‚úÖ Optimized for speed
                    guide_scale=request.sample_guide_scale,
                    seed=request.seed,
                    offload_model=request.offload_model
                )

            keyframes.append(batch_output)

    # Convert keyframes to NumPy for interpolation
    keyframes_np = np.array([frame.cpu().numpy() for frame in torch.cat(keyframes)])

    # ‚úÖ Async Save to Avoid Disk Bottleneck
    output_file = f"{request.task}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
    Thread(target=cache_video, args=(torch.tensor(keyframes_np), output_file, request.fps)).start()

    logging.info(f"‚úÖ Video saved: {output_file}")
    end_time = time.time()
    logging.info(f"‚è±Ô∏è Video generation time: {end_time - start_time} seconds")
    return output_file

# ========== API Endpoints ==========
@app.post("/generate/")
async def generate_api(request: VideoRequest):
    try:
        logging.info(f"üåê Received Request: {request.dict()}")
        video_path = generate_video(request)
        return {"output_path": video_path}
    except Exception as e:
        logging.error(f"‚ùå Error generating video: {e}")
        raise HTTPException(status_code=500, detail=str(e))
