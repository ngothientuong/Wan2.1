# Optimization libraries for WAN 2.1 on A100 GPUs
flash-attn==2.3.3        # Optimized transformer attention (Use compatible version)
xformers>=0.0.26.post1   # Faster attention ops
bitsandbytes>=0.43.3     # 8-bit quantization
torch-tensorrt>=2.6.0    # PyTorch-TensorRT compatibility
deepspeed>=0.10.0        # Multi-GPU training & inference
triton>=2.1.0            # Kernel fusion optimization
nvidia-pyindex           # Required for NVIDIA package indexing
py3nvml                  # GPU monitoring tools
fastertransformer        # NVIDIAâ€™s Transformer optimization lib (optional)