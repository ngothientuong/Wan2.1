# Optimization libraries
xformers==0.0.26.post1   # Efficient transformer attention
bitsandbytes==0.43.3     # 8-bit and 4-bit quantization for speed
flash-attn==2.6.3        # Faster transformer inference
torch-tensorrt==2.6.0    # PyTorch-TensorRT compatibility
rife-interp==1.2.0       # Optical Flow frame interpolation
