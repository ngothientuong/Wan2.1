Keeping all the optimization needed in mind.
The optimizations are utilization of resources (memory, GPUs, CPUs, disk i/o, etc...) and ensure video production time is under 30 min even for 	A100-80G-PCIe with 2 GPUs without losing quality for 1 hour long video
Evaluate time for production after optimization for 2,4,8 GPUs in the same instance



Now, really dig up both repos and review EVERY FILE from both
Mine: https://github.com/ngothientuong/Wan2.1
Source code: https://github.com/Wan-Video/Wan2.1

Tell me again, if I'm fully abiding by the source code for app.py, DockerFile, additional-requirements? Is there any other files or parameters I'm missing? Remember, this is supposed to ensure optimizations!
Will install them from precompiled source and isolate them helps avoid incompability? Will they work on the base image I provided?
Keeping all the optimization needed in mind.
The optimizations are utilization of resources (memory, GPUs, CPUs, disk i/o, etc...) and ensure video production time is under 30 min even for 	A100-80G-PCIe with 2 GPUs without losing quality for 1 hour long video
Ensure no packages incompability nor conflicts and check against what's from the base images as well
Evaluate time for production after optimization for 2,4,8 GPUs in the same instance

Important: Do not assume anything or generate something from your general understanding like last time! Go do review on every file to prove you're better than other models. They've been giving better reponse and actually review every file and didn't blame on resource intensity on their end!
Check files additional-requirements, app.py, DockerFile, and source file requirements.txt as well there , see if anything we can speed up the install as well as making sure they are compatible and utilize all features in source generate.py

Here are the publically accessible repo:
Mine: https://github.com/ngothientuong/Wan2.1
Source code: https://github.com/Wan-Video/Wan2.1
Is my `app.py` good now? It must abide by source code in folder WAN2.1-source-code and the base Docker image , optimize for time, speed, resource & quality, utilize all features similarly like in generate.py params?
Does `app.py` also include all of these below features, especially video-to-audio
‚úÖ Confirmed Features in Wan2.1
üìú Text-to-Video (T2V) ‚Üí Generate videos from text prompts.
üñºÔ∏è Image-to-Video (I2V) ‚Üí Convert static images into dynamic videos.
üñçÔ∏è Text-to-Image (T2I) ‚Üí Create static images from text prompts.
üéµ Video-to-Audio (V2A) ‚Üí Generate sound effects & background music for videos.   [TO-DO]
üìù Visual Text Generation ‚Üí Render English & Chinese text within videos.
(Not needed now, don't include) üìñ Prompt Extension ‚Üí Enhance text prompts using Local Qwen or DashScope API.
‚öôÔ∏è Configurable Parameters ‚Üí Video resolution, frame count, and sampling settings (e.g., 1280x720, 832x480).
These are implemented in the generate.py script, which serves as the main interface for generation tasks, controlled via command-line arguments like --task, --size, and --frame_num.


requirements.txt
```
# Base requirements
torch>=2.4.0
torchvision>=0.19.0
opencv-python>=4.9.0.80
diffusers>=0.31.0
transformers>=4.49.0
tokenizers>=0.20.3
accelerate>=1.1.1
tqdm
imageio
easydict
ftfy
dashscope
imageio-ffmpeg
gradio>=5.0.0
numpy>=1.23.5,<2

# Additional requirements for optimization
flash_attn=2.3.3
xformers>=0.0.26.post1   # Faster attention ops
bitsandbytes>=0.43.3     # 8-bit quantization
torch-tensorrt>=2.6.0    # PyTorch-TensorRT compatibility
deepspeed>=0.10.0        # Multi-GPU training & inference
triton>=2.1.0            # Kernel fusion optimization
nvidia-pyindex           # Required for NVIDIA package indexing
py3nvml                  # GPU monitoring tools
fastertransformer        # NVIDIA‚Äôs Transformer optimization lib (optional)
```

generate.py:
```
parser = argparse.ArgumentParser(
        description="Generate a image or video from a text prompt or image using Wan"
    )
    parser.add_argument(
        "--task",
        type=str,
        default="t2v-14B",
        choices=list(WAN_CONFIGS.keys()),
        help="The task to run.")
    parser.add_argument(
        "--size",
        type=str,
        default="1280*720",
        choices=list(SIZE_CONFIGS.keys()),
        help="The area (width*height) of the generated video. For the I2V task, the aspect ratio of the output video will follow that of the input image."
    )
    parser.add_argument(
        "--frame_num",
        type=int,
        default=None,
        help="How many frames to sample from a image or video. The number should be 4n+1"
    )
    parser.add_argument(
        "--ckpt_dir",
        type=str,
        default=None,
        help="The path to the checkpoint directory.")
    parser.add_argument(
        "--offload_model",
        type=str2bool,
        default=None,
        help="Whether to offload the model to CPU after each model forward, reducing GPU memory usage."
    )
    parser.add_argument(
        "--ulysses_size",
        type=int,
        default=1,
        help="The size of the ulysses parallelism in DiT.")
    parser.add_argument(
        "--ring_size",
        type=int,
        default=1,
        help="The size of the ring attention parallelism in DiT.")
    parser.add_argument(
        "--t5_fsdp",
        action="store_true",
        default=False,
        help="Whether to use FSDP for T5.")
    parser.add_argument(
        "--t5_cpu",
        action="store_true",
        default=False,
        help="Whether to place T5 model on CPU.")
    parser.add_argument(
        "--dit_fsdp",
        action="store_true",
        default=False,
        help="Whether to use FSDP for DiT.")
    parser.add_argument(
        "--save_file",
        type=str,
        default=None,
        help="The file to save the generated image or video to.")
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="The prompt to generate the image or video from.")
    parser.add_argument(
        "--use_prompt_extend",
        action="store_true",
        default=False,
        help="Whether to use prompt extend.")
    parser.add_argument(
        "--prompt_extend_method",
        type=str,
        default="local_qwen",
        choices=["dashscope", "local_qwen"],
        help="The prompt extend method to use.")
    parser.add_argument(
        "--prompt_extend_model",
        type=str,
        default=None,
        help="The prompt extend model to use.")
    parser.add_argument(
        "--prompt_extend_target_lang",
        type=str,
        default="zh",
        choices=["zh", "en"],
        help="The target language of prompt extend.")
    parser.add_argument(
        "--base_seed",
        type=int,
        default=-1,
        help="The seed to use for generating the image or video.")
    parser.add_argument(
        "--image",
        type=str,
        default=None,
        help="The image to generate the video from.")
    parser.add_argument(
        "--sample_solver",
        type=str,
        default='unipc',
        choices=['unipc', 'dpm++'],
        help="The solver used to sample.")
    parser.add_argument(
        "--sample_steps", type=int, default=None, help="The sampling steps.")
    parser.add_argument(
        "--sample_shift",
        type=float,
        default=None,
        help="Sampling shift factor for flow matching schedulers.")
    parser.add_argument(
        "--sample_guide_scale",
        type=float,
        default=5.0,
        help="Classifier free guidance scale.")

    args = parser.parse_args()
```
Dockerfile:
```
# Use NVIDIA‚Äôs Latest PyTorch Image (Ensure CUDA version matches your system)
FROM nvcr.io/nvidia/pytorch:24.01-py3

# Set Environment Variables for Performance Optimization
ENV DEBIAN_FRONTEND=noninteractive
ENV HF_HOME="/root/.cache/huggingface"
ENV MODEL_DIR="/models/Wan2.1-T2V-14B"
ENV TORCH_HOME="/root/.cache/torch"
ENV PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
ENV XFORMERS_FORCE_DISABLE_TRITON=1
ENV TF32_MATMUL=1
ENV CUDA_LAUNCH_BLOCKING=0
ENV PYTHONUNBUFFERED=1

# Install System Dependencies (Ensure CUDA Development Tools are Installed)
RUN apt-get update && apt-get install -y \
  python3-pip git wget curl ffmpeg libgl1-mesa-glx \
  ninja-build build-essential cmake \
  && rm -rf /var/lib/apt/lists/*

# Upgrade pip and install core dependencies
RUN pip install --upgrade pip setuptools wheel packaging

# Copy Requirements Files
COPY requirements.txt /app/requirements.txt
COPY additional-requirements.txt /app/additional-requirements.txt

# Install Python Dependencies (Using Cached Layers)
RUN pip install --no-cache-dir -r /app/requirements.txt

# Install Flash-Attn with CUDA Compatibility
RUN pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3+cu124torch2.5-cp312-cp312-linux_x86_64.whl

# Pre-download WAN 2.1 Model
RUN huggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir ${MODEL_DIR} --revision main

# Set Working Directory & Copy Application Files
WORKDIR /app
COPY . /app

# Expose API Port
EXPOSE 8000

# Run Application
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

app.py
```
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, validator, root_validator
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import time
import os
from threading import Thread
import cv2
import numpy as np
import imageio_ffmpeg as ffmpeg
from wan import WanT2V
from wan.configs import WAN_CONFIGS, SIZE_CONFIGS
import asyncio

try:
    from raft import RAFT
    from raft.utils.utils import InputPadder
    raft_model = RAFT()
    raft_model.load_model("/app/raft-things.pth")  # Ensure the model file exists
    USE_RAFT = True
    logging.info("‚úÖ RAFT model loaded successfully.")
except Exception as e:
    USE_RAFT = False
    logging.warning(f"‚ö†Ô∏è RAFT not available, falling back to OpenCV optical flow: {e}")

# Configure logging
logging.basicConfig(level=logging.INFO)
app = FastAPI()

# Device configuration
NUM_GPUS = torch.cuda.device_count()
DEVICE = "cuda" if NUM_GPUS > 0 else "cpu"

# Model cache to avoid reloading
MODEL_CACHE = {}

def get_optimal_batch_size():
    """Dynamically adjust batch size based on available GPU memory."""
    total_memory = torch.cuda.get_device_properties(0).total_memory
    if total_memory > 40e9:  # A100 80GB
        return 128
    elif total_memory > 20e9:  # A40, RTX 3090
        return 64
    else:
        return 32  # Safe default

def load_model(task, ckpt_dir, ulysses_size=1, ring_size=1, t5_fsdp=False, dit_fsdp=False):
    """Load or retrieve a cached WanT2V model."""
    key = (task, ulysses_size, ring_size, t5_fsdp, dit_fsdp)
    if key not in MODEL_CACHE:
        logging.info(f"üîÑ Loading {task} model on {DEVICE}...")
        config = WAN_CONFIGS[task]
        model = WanT2V(config=config, checkpoint_dir=ckpt_dir, ulysses_size=ulysses_size, ring_size=ring_size).to(DEVICE).half()
        model = torch.compile(model, backend="tensorrt")

        if NUM_GPUS > 1:
            model = nn.DataParallel(model)

        MODEL_CACHE[key] = model
    return MODEL_CACHE[key]

class VideoRequest(BaseModel):
    """Request model with validation and task-specific defaults."""
    task: str
    prompt: str
    size: str = "1280*720"
    num_frames: int = 320
    fps: int = 16
    seed: int = 42
    ckpt_dir: str
    offload_model: bool = False
    t5_cpu: bool = False
    t5_fsdp: bool = False
    dit_fsdp: bool = False
    ulysses_size: int = 1
    ring_size: int = 1
    sample_solver: str = "unipc"
    sample_steps: int = None
    sample_shift: float = 3.0
    sample_guide_scale: float = 3.0
    save_file: str = None
    image: str = None  # For I2V & T2I
    background_music: bool = True  # ‚úÖ Enabled by default
    overlay_text: str = None  # ‚úÖ Default text overlay - Can be "Generated by Wan2.1 AI" or whatever watermark text which sticks through all frames
    use_prompt_extend: bool = False  # ‚úÖ Disabled by default
    prompt_extend_method: str = "local_qwen"
    prompt_extend_model: str = None
    prompt_extend_target_lang: str = "en"
    keyframe_interval: int = 30

    @validator('task')
    def task_must_be_valid(cls, v):
        if v not in WAN_CONFIGS:
            raise ValueError(f"Invalid task: {v}")
        return v

    @validator('size')
    def size_must_be_valid(cls, v):
        if v not in SIZE_CONFIGS:
            raise ValueError(f"Invalid size: {v}")
        return v

    @validator('num_frames')
    def num_frames_must_be_4n_plus_1(cls, v):
        if (v - 1) % 4 != 0:
            v = ((v // 4) * 4) + 1
            logging.warning(f"‚ö†Ô∏è Adjusted num_frames to {v} to satisfy 4n+1 constraint.")
        return v

    @root_validator(pre=False)
    def set_sample_steps_default(cls, values):
        """Set sample_steps based on task if not provided."""
        if values.get('sample_steps') is None:
            task = values.get('task')
            if task.startswith('t2v'):
                values['sample_steps'] = 50
            elif task.startswith('i2v'):
                values['sample_steps'] = 40
        return values

def extend_prompt(prompt, method, model, target_lang):
    """Extend the input prompt using Local Qwen (default)."""
    return f"{prompt}"  # ‚úÖ Keeps the original prompt without extra junk

def preprocess_image(image):
    """Convert image input into a PyTorch tensor for I2V tasks."""
    img = cv2.imread(image) if isinstance(image, str) else np.array(image)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return torch.from_numpy(img).permute(2, 0, 1).float().to(DEVICE) / 255.0

def raft_interpolation(keyframes, interval):
    """Interpolate frames using RAFT (GPU-accelerated optical flow)."""
    interpolated_frames = []

    for i in range(len(keyframes) - 1):
        frame1 = torch.tensor(keyframes[i]).permute(2, 0, 1).unsqueeze(0).float().cuda()
        frame2 = torch.tensor(keyframes[i + 1]).permute(2, 0, 1).unsqueeze(0).float().cuda()

        padder = InputPadder(frame1.shape)
        frame1, frame2 = padder.pad(frame1, frame2)

        with torch.no_grad():
            flow = raft_model(frame1, frame2, iters=20, test_mode=True)[0]  # Optical flow from frame1 ‚Üí frame2

        H, W = frame1.shape[2:]  # Height & Width

        for j in range(1, interval):
            alpha = j / interval

            # Generate flow grid for warping
            grid_x, grid_y = torch.meshgrid(torch.arange(W), torch.arange(H))
            grid_x = grid_x.cuda().float()
            grid_y = grid_y.cuda().float()

            # Apply optical flow scaling based on alpha (progress)
            new_x = grid_x + alpha * flow[0, 0]  # X displacement
            new_y = grid_y + alpha * flow[0, 1]  # Y displacement

            # Normalize coordinates for grid_sample()
            new_x = (2.0 * new_x / (W - 1)) - 1.0
            new_y = (2.0 * new_y / (H - 1)) - 1.0
            grid = torch.stack((new_x, new_y), dim=-1).unsqueeze(0)

            # Warp frame1 towards frame2 using flow-based grid
            interp_frame = F.grid_sample(frame1, grid, mode="bilinear", padding_mode="border", align_corners=True)
            interp_frame = interp_frame.squeeze(0).permute(1, 2, 0).cpu().numpy()
            interpolated_frames.append(interp_frame)

    return interpolated_frames

def opencv_interpolation(keyframes, interval):
    """Interpolate frames using OpenCV's Farneback method for optical flow."""
    interpolated_frames = []
    for i in range(len(keyframes) - 1):
        frame1 = keyframes[i]
        frame2 = keyframes[i + 1]
        flow = cv2.calcOpticalFlowFarneback(cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY),
                                            cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY),
                                            None, 0.5, 3, 15, 3, 5, 1.2, 0)
        for j in range(1, interval):
            alpha = j / interval
            interp_frame = cv2.addWeighted(frame1, 1 - alpha, frame2, alpha, 0)
            interpolated_frames.append(interp_frame)
    return interpolated_frames

def interpolate_frames(keyframes, interval):
    """Choose RAFT or OpenCV for frame interpolation."""
    if USE_RAFT:
        logging.info("üöÄ Using RAFT for GPU-accelerated frame interpolation.")
        return raft_interpolation(keyframes, interval)
    else:
        logging.info("‚ö†Ô∏è RAFT not available, using OpenCV optical flow instead.")
        return opencv_interpolation(keyframes, interval)

def save_video_async(frames, output_file, fps, audio_path=None):
    """Save video frames asynchronously with error handling."""
    try:
        writer = ffmpeg.write_frames(output_file, (1280, 720), fps=fps)
        writer.send(None)
        for frame in frames:
            writer.send(frame)
        writer.close()

        if audio_path:
            os.system(f"ffmpeg -i {output_file} -i {audio_path} -c:v copy -c:a aac {output_file.replace('.mp4', '_with_audio.mp4')}")

    except Exception as e:
        logging.error(f"‚ùå Error saving video: {e}")

def generate_video(request: VideoRequest):
    """Generate video based on the request parameters."""
    start_time = time.time()
    torch.manual_seed(request.seed)

    model = load_model(request.task, request.ckpt_dir, request.ulysses_size, request.ring_size, request.t5_fsdp, request.dit_fsdp)

    # Handle T2I task right here
    if request.task == 't2i':
        with torch.no_grad():  # No gradients needed for inference
            image = model.generate(prompts=[request.prompt], num_frames=1, size=request.size, device=DEVICE)[0]
        output_file = request.save_file or f"output_image_{int(time.time())}.png"
        image_np = image.cpu().numpy()
        # Normalize to 0-255 if needed, then save as PNG
        image_np = (image_np * 255).astype(np.uint8) if image_np.max() <= 1 else image_np
        cv2.imwrite(output_file, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))
        return {"output_path": output_file, "time_seconds": time.time() - start_time}

    # Rest of your video generation logic for 't2v' and 'i2v' goes here
    # (e.g., keyframes, interpolation, audio, etc.)
    keyframe_interval = request.keyframe_interval
    num_keyframes = request.num_frames // keyframe_interval
    batch_size = get_optimal_batch_size()
    num_batches = (num_keyframes + batch_size - 1) // batch_size

    prompt = request.prompt
    keyframes = []

    for i in range(num_batches):
        batch_prompts = [prompt] * min(batch_size, num_keyframes - i * batch_size)

        with torch.no_grad():
            if request.task.startswith('i2v') and request.image:
                image_tensor = preprocess_image(request.image)
                batch_frames = model.generate(prompts=batch_prompts, num_frames=1, size=request.size, device=DEVICE, image=image_tensor)
            else:
                batch_frames = model.generate(prompts=batch_prompts, num_frames=1, size=request.size, device=DEVICE)

        keyframes.extend(batch_frames)

    all_frames = []
    for i in range(len(keyframes) - 1):
        all_frames.append(keyframes[i])
        all_frames.extend(interpolate_frames([keyframes[i], keyframes[i + 1]], keyframe_interval))
    all_frames.append(keyframes[-1])

    if request.background_music:
        logging.info("üéµ Generating Background Music...")
        audio_path = model.generate_audio(prompt=prompt)
    else:
        audio_path = None

    if request.overlay_text:
        logging.info("üìù Overlaying Text on Video...")
        all_frames = apply_text_overlay(all_frames, request.overlay_text)

    output_file = request.save_file or f"output_{int(time.time())}.mp4"
    Thread(target=save_video_async, args=(all_frames, output_file, request.fps, audio_path)).start()

    return {"output_path": output_file, "audio_path": audio_path, "time_seconds": time.time() - start_time}

def apply_text_overlay(frames, text, position="bottom", font_scale_factor=0.05):
    """Apply text overlay to each frame with improved scaling, positioning, and readability."""
    if not text:
        return frames  # Skip if no text is provided

    overlaid_frames = []
    for frame in frames:
        # Convert frame to NumPy array if it's a tensor
        if isinstance(frame, torch.Tensor):
            frame_np = frame.cpu().numpy()
            frame_np = (frame_np * 255).astype(np.uint8) if frame_np.max() <= 1 else frame_np
            frame_np = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)
        else:
            frame_np = frame

        # Get frame dimensions
        height, width, _ = frame_np.shape

        # Dynamically adjust font scale based on video resolution
        font_scale = max(1, int(font_scale_factor * height / 30))  # Auto-scale font size
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_color = (255, 255, 255)  # White text
        thickness = max(1, int(font_scale * 1.5))  # Scale thickness dynamically

        # Get text size to position dynamically
        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
        text_width, text_height = text_size

        # Dynamic text positioning
        if position == "top":
            x, y = 50, 50 + text_height
        elif position == "bottom":
            x, y = (width - text_width) // 2, height - 50
        else:  # Default: center
            x, y = (width - text_width) // 2, (height + text_height) // 2

        # Add semi-transparent background for better readability
        overlay = frame_np.copy()
        bg_x1, bg_y1 = x - 10, y - text_height - 10
        bg_x2, bg_y2 = x + text_width + 10, y + 10
        cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)
        alpha = 0.5  # Transparency level
        frame_np = cv2.addWeighted(overlay, alpha, frame_np, 1 - alpha, 0)

        # Draw text on frame
        cv2.putText(frame_np, text, (x, y), font, font_scale, font_color, thickness, cv2.LINE_AA)

        # Convert back to RGB
        frame_rgb = cv2.cvtColor(frame_np, cv2.COLOR_BGR2RGB)
        overlaid_frames.append(frame_rgb)

    return overlaid_frames

@app.post("/generate/")
async def generate_api(request: VideoRequest):
    """Asynchronous endpoint for video generation."""
    try:
        result = await asyncio.to_thread(generate_video, request)
        return result
    except Exception as e:
        logging.error(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

requirements.txt
```
## Core Requirements
torch>=2.4.0
torchvision>=0.19.0
opencv-python>=4.9.0.80
diffusers>=0.31.0
transformers>=4.49.0
tokenizers>=0.20.3
accelerate>=1.1.1
tqdm
imageio
easydict
ftfy
imageio-ffmpeg
numpy>=1.23.5,<2

## Optimization Libraries for WAN 2.1 on A100 GPUs
xformers>=0.0.26.post1   # Faster attention ops
bitsandbytes>=0.43.3     # 8-bit quantization
torch-tensorrt>=2.6.0    # PyTorch-TensorRT compatibility
deepspeed>=0.10.0        # Multi-GPU training & inference
triton>=2.1.0            # Kernel fusion optimization
fastapi>=0.115.0  # Required for API
pydantic>=2.9.2   # Required for validation
## Optional dependencies (commented out if not explicitly used in app.py)
# fastertransformer        # NVIDIA‚Äôs Transformer optimization lib (not used in app.py)
# nvidia-pyindex           # Required for NVIDIA package indexing

# FlashAttention must be installed separately due to CUDA compatibility issues
```

From above `app.py` and `requirements.txt`, do I have all needed packages?
It must abide by source code in folder WAN2.1-source-code and the base Docker image , optimize for time, speed, resource & quality, utilize all features similarly like in generate.py params?